{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcellation\n",
    "After finishing pre-processing, we perform parcellation with the A424 atlas and process the data in different normalization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from random import randint, seed\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from transformers import ViTImageProcessor, ViTMAEConfig\n",
    "from brainlm_mae.modeling_vit_mae_with_padding import ViTMAEForPreTraining \n",
    "from brainlm_mae.replace_vitmae_attn_with_flash_attn import replace_vitmae_attn_with_flash_attn\n",
    "from utils.utils import convert_fMRIvols_to_A424, process_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = \"/path/to/raw_fMRI_data\"\n",
    "save_data_dir = \"/path/to/a424_fMRI_data\" #Make sure this directory exists.\n",
    "args = {\n",
    "    \"ts_data_dir\": \"/path/to/a424_fMRI_data\",     # \"Path to directory containing dat files, A424 coordinates file, and A424 excel sheet.\",\n",
    "    \"processed_data_dir\": os.path.join(save_data_dir, \"processed\"),     # \"The directory where you want to save the output arrow datasets.\"\n",
    "    \"dataset_name\": \"xxx\",\n",
    "    \"metadata_path\": \"path/to/metadata.csv\"\n",
    "}\n",
    "\n",
    "# Convert fMRI volumes to A424 time-series data\n",
    "convert_fMRIvols_to_A424(data_path=raw_data_dir, output_path=save_data_dir)\n",
    "\n",
    "# Processing datasets\n",
    "process_datasets(args, args[\"processed_data_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "## Setup args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "moving_window_len = 200\n",
    "kfold = 5\n",
    "batch_size = 8\n",
    "data_path = \"/path/to/a424_fMRI_data/processed\"\n",
    "output_path = \"output/\"\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "output_path = os.path.join(output_path, dt_string)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def preprocess_fmri(examples, recording_col_name, variable_of_interest_col_name=\"Response\", moving_window_len=200):\n",
    "    \"\"\"\n",
    "    Preprocessing function for dataset samples. This function is passed into Trainer as\n",
    "    a preprocessor which takes in one row of the loaded dataset and constructs a model\n",
    "    input sample according to the arguments which model.forward() expects.\n",
    "\n",
    "    The reason this function is defined inside on main() function is because we need\n",
    "    access to arguments such as cell_expression_vector_col_name.\n",
    "    \"\"\"\n",
    "    label = examples[variable_of_interest_col_name][0]\n",
    "    brain_net = examples[\"Brain_Network\"]\n",
    "    if math.isnan(label):\n",
    "        label = -1\n",
    "    else:\n",
    "        label = int(label)\n",
    "    label = torch.tensor(label, dtype=torch.int64)\n",
    "    signal_vector = examples[recording_col_name]\n",
    "    signal_vector = torch.tensor(signal_vector, dtype=torch.float32)\n",
    "\n",
    "    # Choose random starting index, take window of moving_window_len points for each region\n",
    "    start_idx = random.randint(0, signal_vector.shape[1] - moving_window_len)\n",
    "    end_idx = start_idx + moving_window_len\n",
    "    signal_window = signal_vector[:, start_idx: end_idx]\n",
    "    \n",
    "    \n",
    "    # Append signal values and coords\n",
    "    window_xyz_list = []\n",
    "    for brain_region_idx in range(signal_window.shape[0]):\n",
    "\n",
    "        # Append voxel coordinates\n",
    "        xyz = torch.tensor([\n",
    "            coords_ds[brain_region_idx][\"X\"],\n",
    "            coords_ds[brain_region_idx][\"Y\"],\n",
    "            coords_ds[brain_region_idx][\"Z\"]\n",
    "        ], dtype=torch.float32)\n",
    "        window_xyz_list.append(xyz)\n",
    "    window_xyz_list = torch.stack(window_xyz_list)\n",
    "\n",
    "    # Add in key-value pairs for model inputs which CellLM is expecting in forward() function:\n",
    "    #  signal_vectors and xyz_vectors\n",
    "    #  These lists will be stacked into torch Tensors by collate() function (defined above).\n",
    "    examples[\"signal_vectors\"] = signal_window.unsqueeze(0)\n",
    "    examples[\"xyz_vectors\"] = window_xyz_list.unsqueeze(0)\n",
    "    examples[\"brain_network\"] = np.array(brain_net)\n",
    "    examples[\"label\"] = label\n",
    "    return examples\n",
    "\n",
    "class fMRIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, recording_col_name, variable_of_interest_col_name, moving_window_len=200):\n",
    "        self.dataset = dataset\n",
    "        self.recording_col_name = recording_col_name\n",
    "        self.variable_of_interest_col_name = variable_of_interest_col_name\n",
    "        self.moving_window_len = moving_window_len\n",
    "        self.features, self.labels = self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows\n",
    "    \n",
    "    def _load_data(self):\n",
    "        features = []\n",
    "        labels = []\n",
    "        for recording_idx in range(self.dataset.num_rows):\n",
    "            example1 = concat_ds[recording_idx]\n",
    "            features.append(example1)\n",
    "            labels.append(example1[self.variable_of_interest_col_name])\n",
    "            # Wrap each value in the key:value pairs into a list (expected by preprocess() and collate())\n",
    "            \n",
    "        return features, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.features[idx]\n",
    "        processed_example = preprocess_fmri(example, self.recording_col_name, moving_window_len=self.moving_window_len)\n",
    "\n",
    "        return {\n",
    "            \"signal_vectors\": processed_example[\"signal_vectors\"].squeeze(0),\n",
    "            \"xyz_vectors\": processed_example[\"xyz_vectors\"].squeeze(0),\n",
    "            \"input_ids\": processed_example[\"signal_vectors\"].squeeze(0),\n",
    "            \"labels\": processed_example[\"label\"].squeeze(0),\n",
    "            \"brain_network\": processed_example[\"brain_network\"]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def collate_fn(example):\n",
    "    \"\"\"\n",
    "    This function tells the dataloader how to stack a batch of examples from the dataset.\n",
    "    Need to stack gene expression vectors and maintain same argument names for model inputs\n",
    "    which CellLM is expecting in forward() function:\n",
    "        expression_vectors, sampled_gene_indices, and cell_indices\n",
    "    \"\"\"\n",
    "    # These inputs will go to model.forward(), names must match\n",
    "    return {\n",
    "        \"signal_vectors\": torch.stack([e[\"signal_vectors\"] for e in example]),\n",
    "        \"xyz_vectors\": torch.stack([e[\"xyz_vectors\"] for e in example]),\n",
    "        \"input_ids\": torch.stack([e[\"signal_vectors\"] for e in example]),\n",
    "        \"labels\": torch.stack([e[\"labels\"] for e in example]),\n",
    "        \"brain_network\": torch.stack([torch.tensor(e[\"brain_network\"]) for e in example])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "train_ds = load_from_disk(os.path.join(data_path, \"data\"))\n",
    "coords_ds = load_from_disk(os.path.join(data_path, \"Brain_Region_Coordinates\"))\n",
    "concat_ds = concatenate_datasets([train_ds])\n",
    "\n",
    "variable_of_interest_col_name = \"Response\"\n",
    "col_name = \"Raw_Recording\"\n",
    "dataset = fMRIDataset(concat_ds, col_name, \"Response\", moving_window_len=moving_window_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainlm_mae.modeling_brainlm import BrainLMForPretraining\n",
    "class MultimodalfMRI(nn.Module):\n",
    "    def __init__(self, resnet):\n",
    "        super(MultimodalfMRI, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        self.classifier = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, signal_vectors, vit_cls_token):\n",
    "        resnet_input = torch.bmm(signal_vectors, signal_vectors.permute(0, 2, 1)).unsqueeze(1).repeat(1, 3, 1, 1).float()\n",
    "        cls_token = torch.concat([vit_cls_token, self.resnet(resnet_input)], dim=1)\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits\n",
    "    \n",
    "model = BrainLMForPretraining.from_pretrained(\"pretrained_models/brainlm\")\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.vit.embeddings.mask_ratio = 0.0\n",
    "model.vit.embeddings.config.mask_ratio = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "metric_df = pd.DataFrame(columns=[\"kfold\", \"epoch\", \"F1\", \"Accuracy\", \"BACC\", \"ROC AUC\", \"Recall\", \"Precision\", \"Sensitivity\", \"Specificity\", \"MCC\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "for i, (train_idx, val_idx) in enumerate(StratifiedKFold(n_splits=kfold, shuffle=False).split(dataset.features, dataset.labels)):\n",
    "            \n",
    "    model_resnet = resnet18(pretrained=False)\n",
    "    model_resnet.fc = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(model_resnet.fc.in_features, 256),\n",
    "    )\n",
    "    model_resnet = MultimodalfMRI(model_resnet).to(device)\n",
    "    optimizer = torch.optim.AdamW(model_resnet.parameters(), lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-7)\n",
    "    \n",
    "\n",
    "    train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    best_mcc = 0\n",
    "    best_epoch = 0\n",
    "    model.eval()\n",
    "    loop = tqdm(range(100))\n",
    "    for epoch in loop:\n",
    "        loop.set_description(f\"Fold: {i} Epoch {epoch}\")\n",
    "        metrics = []\n",
    "        metrics.append(i)\n",
    "        optimizer.zero_grad()\n",
    "        model_resnet.train()\n",
    "        \n",
    "        for example in trainloader:\n",
    "            with torch.no_grad():\n",
    "                encoder_output = model.vit(\n",
    "                    signal_vectors=example[\"signal_vectors\"].to(device),\n",
    "                    xyz_vectors=example[\"xyz_vectors\"].to(device),\n",
    "                    output_attentions=True,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            logits = model_resnet(example[\"signal_vectors\"].to(device), encoder_output.last_hidden_state[:,0,:])\n",
    "            loss = nn.CrossEntropyLoss()(logits, example[\"labels\"].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model_resnet.eval()\n",
    "        \n",
    "        losses = []\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        for example in valloader:\n",
    "            with torch.no_grad():\n",
    "                encoder_output = model.vit(\n",
    "                    signal_vectors=example[\"signal_vectors\"].to(device),\n",
    "                    xyz_vectors=example[\"xyz_vectors\"].to(device),\n",
    "                    output_attentions=True,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                logits = model_resnet(example[\"signal_vectors\"].to(device), encoder_output.last_hidden_state[:,0,:])\n",
    "                loss = nn.CrossEntropyLoss()(logits, example[\"labels\"].to(device))\n",
    "                losses.append(loss.item())\n",
    "                logits_list.append(logits.detach().cpu().numpy())\n",
    "                labels_list.append(example[\"labels\"].detach().cpu().numpy())\n",
    "        logits_list = np.concatenate(logits_list, axis=0)\n",
    "        labels_list = np.concatenate(labels_list)\n",
    "        preds_list = np.argmax(logits_list, axis=1)\n",
    "        acc = accuracy_score(labels_list, preds_list)\n",
    "        bacc = balanced_accuracy_score(labels_list, preds_list)\n",
    "        f1 = f1_score(labels_list, preds_list, average='macro')\n",
    "        roc_auc = roc_auc_score(labels_list, logits_list[:, 1])\n",
    "        cm = confusion_matrix(labels_list, preds_list)\n",
    "        cm_percent = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "        mcc = matthews_corrcoef(labels_list, preds_list)\n",
    "        tp, fp, tn, fn = cm[1, 1], cm[0, 1], cm[0, 0], cm[1, 0]\n",
    "        metrics.append(epoch)\n",
    "        metrics.append(f1)\n",
    "        metrics.append(acc)\n",
    "        metrics.append(bacc)\n",
    "        metrics.append(roc_auc)\n",
    "        metrics.append(tp/(tp+fn+1e-9))\n",
    "        metrics.append(tp/(tp+fp+1e-9))\n",
    "        metrics.append(tp/(tp+fn+1e-9))\n",
    "        metrics.append(tn/(tn+fp+1e-9))\n",
    "        metrics.append(mcc)\n",
    "        metrics.append(tp)\n",
    "        metrics.append(fp)\n",
    "        metrics.append(tn)\n",
    "        metrics.append(fn)\n",
    "        loop.set_postfix_str(f\"Best MCC: {best_mcc:.4f}\")\n",
    "        metric_df.loc[len(metric_df)] = metrics\n",
    "        \n",
    "        if mcc > best_mcc:\n",
    "            best_mcc = mcc\n",
    "            best_epoch = epoch\n",
    "            save_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"model_resnet\": model_resnet.state_dict(),\n",
    "                \"best_mcc\": best_mcc,\n",
    "                \"best_epoch\": best_epoch\n",
    "            }\n",
    "            torch.save(save_dict, os.path.join(output_path, f\"best_mcc.pt\"))\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"model_resnet\": model_resnet.state_dict(),\n",
    "            \"best_mcc\": best_mcc,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "        torch.save(save_dict, os.path.join(output_path, f\"last_model.pt\"))\n",
    "        scheduler.step()\n",
    "    metric_df.to_csv(os.path.join(output_path, f\"results.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
